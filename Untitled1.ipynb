{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "747835e8-ddee-492d-8595-7ea2bf02b1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "FINAL OPTIMIZED 4-CLASS ENSEMBLE CLASSIFIER\n",
      "===========================================================================\n",
      "\n",
      "Dataset: 1267 rows × 24 columns\n",
      "\n",
      "Merging KTAS classes 4 and 5 into a single class (4)...\n",
      "\n",
      "New Class Distribution:\n",
      "  KTAS 1:   26 (  2.1%)\n",
      "  KTAS 2:  220 ( 17.4%)\n",
      "  KTAS 3:  487 ( 38.4%)\n",
      "  KTAS 4:  534 ( 42.1%)\n",
      "\n",
      "An error occurred: name 'ColumnTransformer' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\sythe\\AppData\\Local\\Temp\\ipykernel_20140\\2109808032.py\", line 132, in <module>\n",
      "    ('preprocessor', ColumnTransformer(\n",
      "                     ^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'ColumnTransformer' is not defined\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, make_scorer, recall_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Keyword-based Feature Engineering Function ---\n",
    "def calculate_severity_score(text):\n",
    "    \"\"\"Enhanced keyword-based severity scoring\"\"\"\n",
    "    score = 0\n",
    "    text = str(text).lower()\n",
    "\n",
    "    critical = {\n",
    "        'arrest': 25, 'unconscious': 20, 'cpr': 25, 'hemorrhage': 18,\n",
    "        'infarction': 18, 'stroke': 20, 'seizure': 16, 'anaphylaxis': 18,\n",
    "        'shock': 20, 'paralysis': 18, 'hemiparesis': 16, 'code': 25\n",
    "    }\n",
    "    high = {\n",
    "        'dyspnea': 12, 'chest pain': 14, 'hematemesis': 13, 'melena': 12,\n",
    "        'syncope': 12, 'bleeding': 12, 'confusion': 13, 'acute': 11\n",
    "    }\n",
    "    moderate = {\n",
    "        'fever': 6, 'vomiting': 6, 'headache': 5, 'dizzy': 5, 'diarrhea': 5,\n",
    "        'laceration': 6, 'burn': 7, 'pain': 3, 'rash': 4\n",
    "    }\n",
    "    \n",
    "    for keyword, value in {**critical, **high, **moderate}.items():\n",
    "        if re.search(r'\\b' + re.escape(keyword) + r'\\b', text):\n",
    "            score += value\n",
    "    \n",
    "    return score\n",
    "\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"Clinical feature engineering\"\"\"\n",
    "    \n",
    "    # Clean vitals\n",
    "    for col in ['SBP', 'DBP', 'HR', 'RR', 'Saturation', 'BT']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].replace(0, np.nan).fillna(df[col].median())\n",
    "    \n",
    "    # Derived vitals\n",
    "    df['shock_index'] = df['HR'] / (df['SBP'] + 1)\n",
    "    df['map'] = (df['SBP'] + 2 * df['DBP']) / 3\n",
    "    df['pulse_pressure'] = df['SBP'] - df['DBP']\n",
    "    \n",
    "    # Count abnormal vitals\n",
    "    df['abnormal_vitals'] = (\n",
    "        ((df['SBP'] < 90) | (df['SBP'] > 180)) +\n",
    "        ((df['HR'] < 50) | (df['HR'] > 120)) +\n",
    "        ((df['RR'] < 10) | (df['RR'] > 24)) +\n",
    "        (df['Saturation'] < 92) +\n",
    "        ((df['BT'] < 36) | (df['BT'] > 38.5))\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Interactions\n",
    "    df['severity_x_pain'] = df['severity_score'] * np.log1p(df['NRS_pain'])\n",
    "    df['severity_x_mental'] = df['severity_score'] * (df['Mental'] != 1).astype(int)\n",
    "    df['age_x_severity'] = (df['Age'] / 100) * df['severity_score']\n",
    "    \n",
    "    # Age risk\n",
    "    df['high_risk_age'] = ((df['Age'] < 2) | (df['Age'] > 70)).astype(int)\n",
    "    \n",
    "    # Pain categories\n",
    "    df['high_pain'] = (df['NRS_pain'] >= 7).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --- Custom Scorer for GridSearchCV to focus on KTAS 1 recall ---\n",
    "ktas1_recall_scorer = make_scorer(recall_score, labels=[0], average='macro', zero_division=0)\n",
    "\n",
    "\n",
    "# --- Main ---\n",
    "try:\n",
    "    print(\"=\"*75)\n",
    "    print(\"FINAL OPTIMIZED 4-CLASS ENSEMBLE CLASSIFIER\")\n",
    "    print(\"=\"*75)\n",
    "    \n",
    "    df = pd.read_csv(r\"C:\\DATASETS\\triage\\data.csv\", delimiter=';', encoding='windows-1254')\n",
    "    print(f\"\\nDataset: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "\n",
    "    # Preprocessing\n",
    "    df.replace('#BOŞ!', np.nan, inplace=True)\n",
    "    \n",
    "    for col in ['KTAS duration_min', 'Length of stay_min']:\n",
    "        if col in df.columns and df[col].dtype == 'object':\n",
    "            df[col] = df[col].str.replace(',', '.').astype(float)\n",
    "    \n",
    "    df['Chief_complain'] = df['Chief_complain'].fillna('')\n",
    "    df['severity_score'] = df['Chief_complain'].apply(calculate_severity_score)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object' and col != 'Chief_complain':\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype in ['int64', 'float64']:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    df.dropna(subset=['KTAS_expert'], inplace=True)\n",
    "\n",
    "    # --- MERGE CLASS 4 AND 5 ---\n",
    "    print(\"\\nMerging KTAS classes 4 and 5 into a single class (4)...\")\n",
    "    df['KTAS_expert'] = df['KTAS_expert'].replace(5, 4)\n",
    "    \n",
    "    df = engineer_features(df)\n",
    "    \n",
    "    print(\"\\nNew Class Distribution:\")\n",
    "    for cls in sorted(df['KTAS_expert'].unique()):\n",
    "        count = (df['KTAS_expert'] == cls).sum()\n",
    "        print(f\"  KTAS {int(cls)}: {count:4d} ({count/len(df)*100:5.1f}%)\")\n",
    "    \n",
    "    # Features\n",
    "    numeric_features = [\n",
    "        'Age', 'Sex', 'Group', 'Arrival mode', 'Injury', 'Mental', \n",
    "        'Pain', 'NRS_pain', 'SBP', 'DBP', 'HR', 'RR', 'BT', 'Saturation',\n",
    "        'severity_score', 'shock_index', 'map', 'pulse_pressure',\n",
    "        'abnormal_vitals', 'severity_x_pain', 'severity_x_mental',\n",
    "        'age_x_severity', 'high_risk_age', 'high_pain'\n",
    "    ]\n",
    "    text_feature = 'Chief_complain'\n",
    "    target = 'KTAS_expert'\n",
    "\n",
    "    # Using a Pipeline to integrate preprocessing and classifier\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', StandardScaler(), numeric_features),\n",
    "                ('text', TfidfVectorizer(stop_words='english', max_features=250)),\n",
    "            ])),\n",
    "        ('classifier', XGBClassifier(\n",
    "            random_state=42,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='mlogloss',\n",
    "            n_jobs=-1))\n",
    "    ])\n",
    "    \n",
    "    # Split Data\n",
    "    X = df.drop(target, axis=1)\n",
    "    y = (df[target].values - 1).astype(int) # 0-indexed for training (0,1,2,3)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Hyperparameter Grid for XGBoost\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [400, 500],\n",
    "        'classifier__max_depth': [5, 7],\n",
    "        'classifier__learning_rate': [0.05],\n",
    "        'classifier__subsample': [0.8, 1.0],\n",
    "        'classifier__colsample_bytree': [0.8]\n",
    "    }\n",
    "\n",
    "    # --- Train Model 1: Generalist (for overall accuracy) ---\n",
    "    print(\"\\n\" + \"=\"*75)\n",
    "    print(\"1. TUNING GENERALIST MODEL FOR BEST OVERALL F1-SCORE\")\n",
    "    print(\"=\"*75)\n",
    "    grid_search_general = GridSearchCV(pipeline, param_grid, cv=3, n_jobs=-1, scoring='f1_weighted', verbose=1)\n",
    "    grid_search_general.fit(X_train, y_train)\n",
    "    best_general_model = grid_search_general.best_estimator_\n",
    "    print(f\"\\nBest generalist parameters found: {grid_search_general.best_params_}\")\n",
    "\n",
    "    # --- Train Model 2: Specialist (for KTAS 1 recall) ---\n",
    "    print(\"\\n\" + \"=\"*75)\n",
    "    print(\"2. TUNING SPECIALIST MODEL TO MAXIMIZE KTAS 1 RECALL\")\n",
    "    print(\"=\"*75)\n",
    "    grid_search_specialist = GridSearchCV(pipeline, param_grid, cv=3, n_jobs=-1, scoring=ktas1_recall_scorer, verbose=1)\n",
    "    grid_search_specialist.fit(X_train, y_train)\n",
    "    best_specialist_model = grid_search_specialist.best_estimator_\n",
    "    print(f\"\\nBest specialist parameters found: {grid_search_specialist.best_params_}\")\n",
    "\n",
    "    # --- Combine Predictions ---\n",
    "    print(\"\\n\" + \"=\"*75)\n",
    "    print(\"3. COMBINING PREDICTIONS FROM GENERALIST & SPECIALIST MODELS\")\n",
    "    print(\"=\"*75)\n",
    "    \n",
    "    # Get standard predictions from the accurate generalist model\n",
    "    y_pred_general = best_general_model.predict(X_test)\n",
    "    \n",
    "    # Get probabilities from the sensitive specialist model\n",
    "    y_pred_proba_specialist = best_specialist_model.predict_proba(X_test)\n",
    "\n",
    "    # Start with the generalist's predictions\n",
    "    final_predictions = y_pred_general.copy()\n",
    "    \n",
    "    # If the specialist model is confident enough about KTAS 1, override the generalist\n",
    "    threshold = 0.28\n",
    "    print(f\"\\nApplying custom prediction threshold of {threshold} for KTAS 1...\")\n",
    "    override_count = 0\n",
    "    for i in range(len(y_test)):\n",
    "        if y_pred_proba_specialist[i][0] >= threshold:\n",
    "            if final_predictions[i] != 0:\n",
    "                override_count += 1\n",
    "            final_predictions[i] = 0 # Predict KTAS 1\n",
    "            \n",
    "    print(f\"Specialist model overrode the generalist's prediction for {override_count} case(s).\")\n",
    "    y_pred = final_predictions\n",
    "    \n",
    "    # --- Results Section ---\n",
    "    print(\"\\n\" + \"=\"*75)\n",
    "    print(\"FINAL ENSEMBLE RESULTS\")\n",
    "    print(\"=\"*75)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    print(f\"\\n{'Metric':<25} {'Score':<15}\")\n",
    "    print(\"-\"*40)\n",
    "    print(f\"{'Accuracy':<25} {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"{'F1-Score (Weighted)':<25} {f1_weighted:.4f}\")\n",
    "    print(f\"{'F1-Score (Macro)':<25} {f1_macro:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*75)\n",
    "    print(\"CLASSIFICATION REPORT\")\n",
    "    print(\"-\"*75)\n",
    "    target_names = [f\"KTAS {i}\" for i in range(1, 5)] # Now 4 classes\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names, zero_division=0))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*75)\n",
    "    print(\"CONFUSION MATRIX\")\n",
    "    print(\"-\"*75)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    classes = sorted(np.unique(y_test))\n",
    "    \n",
    "    print(\"\\n          Predicted KTAS\")\n",
    "    print(\"Actual    \", \"   \".join([f\"{int(c)+1:3d}\" for c in classes]))\n",
    "    print(\"----------------------------------------\")\n",
    "    for i, cls in enumerate(classes):\n",
    "        print(f\" KTAS {int(cls)+1}  {cm[i]}\")\n",
    "\n",
    "except ModuleNotFoundError as e:\n",
    "    if 'xgboost' in str(e):\n",
    "        print(\"\\nERROR: XGBoost not found. Please install it by running: pip install xgboost\")\n",
    "    else:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458bd8ec-2085-4ad8-b29a-c221e2982a55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor2",
   "language": "python",
   "name": "tensor2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
